import streamlit as st
import streamlit.components.v1 as components
import pandas as pd
import plotly.express as px
import json
import random
import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from io import BytesIO

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Kaggle Writeups æµè§ˆå™¨",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Umami ç»Ÿè®¡
components.html("""
<script defer src="https://cloud.umami.is/script.js" data-website-id="64e07ed3-9f6b-4c35-9955-5e2734f07400"></script>
""", height=0)

# å¡ç‰‡æ ·å¼ CSS
st.markdown("""
<style>
.writeup-card {
    border: 1px solid #e0e0e0;
    border-radius: 10px;
    padding: 15px;
    margin-bottom: 15px;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    min-height: 200px;
}
.writeup-card:hover {
    box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    transform: translateY(-2px);
    transition: all 0.3s ease;
}
.card-title {
    font-size: 1.1em;
    font-weight: bold;
    margin-bottom: 8px;
    line-height: 1.3;
}
.card-subtitle {
    font-size: 0.85em;
    opacity: 0.9;
    margin-bottom: 10px;
}
.card-badge {
    display: inline-block;
    padding: 3px 10px;
    border-radius: 15px;
    font-size: 0.75em;
    background: rgba(255,255,255,0.2);
    margin-right: 5px;
}
.card-meta {
    font-size: 0.8em;
    opacity: 0.8;
    margin-top: 10px;
}
</style>
""", unsafe_allow_html=True)

# åˆ†ç±»ä¸­è‹±æ–‡æ˜ å°„
CATEGORY_CN = {
    "Education & Learning": "æ•™è‚²ä¸å­¦ä¹ ",
    "Healthcare & Medical": "åŒ»ç–—å¥åº·",
    "Developer Tools & IDEs": "å¼€å‘å·¥å…·ä¸IDE",
    "Content Creation & Media": "å†…å®¹åˆ›ä½œä¸åª’ä½“",
    "Accessibility & Assistive Technology": "æ— éšœç¢ä¸è¾…åŠ©æŠ€æœ¯",
    "Productivity & Task Management": "æ•ˆç‡ä¸ä»»åŠ¡ç®¡ç†",
    "Research & Scientific Tools": "ç§‘ç ”å·¥å…·",
    "Safety & Security": "å®‰å…¨é˜²æŠ¤",
    "Mental Health & Wellness": "å¿ƒç†å¥åº·",
    "Business Intelligence & Analytics": "å•†ä¸šæ™ºèƒ½ä¸åˆ†æ",
    "Career & Professional Development": "èŒä¸šå‘å±•",
    "Finance & Investment": "é‡‘èæŠ•èµ„",
    "Fitness & Nutrition": "å¥èº«ä¸è¥å…»",
    "Design & Architecture": "è®¾è®¡ä¸å»ºç­‘",
    "Gaming & Entertainment": "æ¸¸æˆå¨±ä¹",
    "Legal & Compliance": "æ³•å¾‹åˆè§„",
    "Environmental & Sustainability": "ç¯ä¿ä¸å¯æŒç»­å‘å±•",
    "E-commerce & Retail": "ç”µå•†é›¶å”®",
    "Emergency & Crisis Management": "åº”æ€¥ç®¡ç†",
    "Food & Culinary": "ç¾é£Ÿçƒ¹é¥ª",
    "Travel & Navigation": "æ—…è¡Œå¯¼èˆª",
    "Language & Translation": "è¯­è¨€ç¿»è¯‘",
    "Agriculture & Farming": "å†œä¸š",
    "Document Processing & Management": "æ–‡æ¡£å¤„ç†ä¸ç®¡ç†",
    "Communication & Collaboration": "æ²Ÿé€šåä½œ",
    "Computer Vision & Image Processing": "è®¡ç®—æœºè§†è§‰ä¸å›¾åƒå¤„ç†",
    "Infrastructure & Cloud Systems": "åŸºç¡€è®¾æ–½ä¸äº‘ç³»ç»Ÿ",
    "Transportation & Logistics": "äº¤é€šç‰©æµ",
    "Real Estate & Property": "æˆ¿åœ°äº§",
    "Smart Home & IoT": "æ™ºèƒ½å®¶å±…ä¸ç‰©è”ç½‘",
}

def get_category_cn(category):
    """è·å–åˆ†ç±»çš„ä¸­æ–‡åç§°"""
    return CATEGORY_CN.get(category, category)

# åŠ è½½æ•°æ®
@st.cache_data
def load_data(csv_path):
    try:
        df = pd.read_csv(csv_path)
        return df
    except Exception as e:
        st.error(f"åŠ è½½æ•°æ®å‡ºé”™: {e}")
        return pd.DataFrame()

def extract_youtube_id(url):
    """ä» URL ä¸­æå– YouTube è§†é¢‘ ID"""
    if pd.isna(url) or not url:
        return None
    patterns = [
        r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([a-zA-Z0-9_-]{11})',
    ]
    for pattern in patterns:
        match = re.search(pattern, str(url))
        if match:
            return match.group(1)
    return None

def render_card(item, idx):
    """æ¸²æŸ“å•ä¸ª Writeup å¡ç‰‡"""
    title = item['title'][:60] + "..." if len(str(item['title'])) > 60 else item['title']
    subtitle = item['description'][:100] + "..." if len(str(item['description'])) > 100 else item['description']

    # æ ¹æ®åˆ†ç±»å“ˆå¸Œå€¼é€‰æ‹©é¢œè‰²
    colors = [
        "linear-gradient(135deg, #667eea 0%, #764ba2 100%)",
        "linear-gradient(135deg, #f093fb 0%, #f5576c 100%)",
        "linear-gradient(135deg, #4facfe 0%, #00f2fe 100%)",
        "linear-gradient(135deg, #43e97b 0%, #38f9d7 100%)",
        "linear-gradient(135deg, #fa709a 0%, #fee140 100%)",
        "linear-gradient(135deg, #a8edea 0%, #fed6e3 100%)",
        "linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%)",
        "linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%)",
    ]
    color_idx = hash(str(item['category'])) % len(colors)
    bg = colors[color_idx]

    st.markdown(f"""
    <div class="writeup-card" style="background: {bg};">
        <div class="card-title">{title}</div>
        <div class="card-subtitle">{subtitle}</div>
        <div>
            <span class="card-badge">ğŸ“‚ {get_category_cn(item['category'])}</span>
            <span class="card-badge">âœ“ {item['confidence']}</span>
        </div>
        <div class="card-meta">ğŸ‘¤ {item['authors']}</div>
    </div>
    """, unsafe_allow_html=True)

    if st.button("æŸ¥çœ‹è¯¦æƒ…", key=f"btn_{idx}_{item['writeup_id']}"):
        st.session_state.selected_id = item['writeup_id']
        st.session_state.view_mode = "detail"
        st.rerun()

def main():
    st.title("ğŸ” Kaggle Writeups æµè§ˆå™¨")

    csv_path = "kaggle_writeups_export/writeups_classified.csv"
    df = load_data(csv_path)

    if df.empty:
        st.warning("æœªæ‰¾åˆ°æ•°æ®ã€‚")
        return

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'page' not in st.session_state:
        st.session_state.page = 0
    if 'view_mode' not in st.session_state:
        st.session_state.view_mode = "cards"
    if 'selected_id' not in st.session_state:
        st.session_state.selected_id = None

    # ä¾§è¾¹æ ç­›é€‰å™¨
    st.sidebar.header("ğŸ›ï¸ ç­›é€‰æ¡ä»¶")

    # åˆ†ç±»ç­›é€‰ - ä½¿ç”¨ä¸­æ–‡æ˜¾ç¤º
    all_categories = sorted(df['category'].dropna().unique().tolist())
    category_options = {get_category_cn(cat): cat for cat in all_categories}
    selected_categories_cn = st.sidebar.multiselect(
        "ğŸ“‚ åˆ†ç±»",
        options=list(category_options.keys()),
        default=[]
    )
    selected_categories = [category_options[cn] for cn in selected_categories_cn]

    # ç½®ä¿¡åº¦ç­›é€‰
    if 'confidence' in df.columns:
        confidence_levels = df['confidence'].unique().tolist()
        selected_confidence = st.sidebar.multiselect(
            "âœ“ ç½®ä¿¡åº¦",
            options=confidence_levels,
            default=[]
        )
    else:
        selected_confidence = []

    # ä½œè€…ç­›é€‰
    all_authors = sorted(df['authors'].dropna().unique().tolist())
    selected_authors = st.sidebar.multiselect(
        "ğŸ‘¤ ä½œè€…",
        options=all_authors[:100],  # é™åˆ¶æ•°é‡ä»¥æé«˜æ€§èƒ½
        default=[]
    )

    # æœç´¢ç­›é€‰
    search_query = st.sidebar.text_input("ğŸ” æœç´¢", "")

    # åº”ç”¨ç­›é€‰æ¡ä»¶
    filtered_df = df.copy()

    if selected_categories:
        filtered_df = filtered_df[filtered_df['category'].isin(selected_categories)]
    if selected_confidence:
        filtered_df = filtered_df[filtered_df['confidence'].isin(selected_confidence)]
    if selected_authors:
        filtered_df = filtered_df[filtered_df['authors'].isin(selected_authors)]
    if search_query:
        filtered_df = filtered_df[
            filtered_df['title'].str.contains(search_query, case=False, na=False) |
            filtered_df['description'].str.contains(search_query, case=False, na=False)
        ]

    # ä¾§è¾¹æ æ“ä½œ
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ² æ“ä½œ")

    # éšæœºæŒ‰é’®
    if st.sidebar.button("ğŸ² éšæœº Writeup", use_container_width=True):
        if not filtered_df.empty:
            random_id = filtered_df.sample(1)['writeup_id'].iloc[0]
            st.session_state.selected_id = random_id
            st.session_state.view_mode = "detail"
            st.rerun()

    # å¯¼å‡ºæŒ‰é’®
    if st.sidebar.button("ğŸ“¥ å¯¼å‡ºç­›é€‰ç»“æœ CSV", use_container_width=True):
        csv_data = filtered_df.to_csv(index=False)
        st.sidebar.download_button(
            "ä¸‹è½½ CSV",
            csv_data,
            file_name="filtered_writeups.csv",
            mime="text/csv"
        )

    # ç»Ÿè®¡æŒ‡æ ‡è¡Œ
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ğŸ“Š æ€»æ•°", len(df))
    col2.metric("ğŸ” ç­›é€‰å", len(filtered_df))
    col3.metric("ğŸ“‚ åˆ†ç±»æ•°", len(filtered_df['category'].unique()))
    col4.metric("ğŸ‘¤ ä½œè€…æ•°", len(filtered_df['authors'].unique()))

    # è§†å›¾æ¨¡å¼åˆ‡æ¢
    view_col1, view_col2, view_col3, view_col4 = st.columns([1, 1, 1, 3])
    with view_col1:
        if st.button("ğŸƒ å¡ç‰‡", use_container_width=True):
            st.session_state.view_mode = "cards"
            st.session_state.selected_id = None
            st.rerun()
    with view_col2:
        if st.button("ğŸ“‹ è¡¨æ ¼", use_container_width=True):
            st.session_state.view_mode = "table"
            st.session_state.selected_id = None
            st.rerun()
    with view_col3:
        if st.button("ğŸ“ˆ ç»Ÿè®¡", use_container_width=True):
            st.session_state.view_mode = "stats"
            st.session_state.selected_id = None
            st.rerun()

    st.markdown("---")

    # è¯¦æƒ…è§†å›¾
    if st.session_state.view_mode == "detail" and st.session_state.selected_id:
        if st.button("â† è¿”å›åˆ—è¡¨"):
            st.session_state.view_mode = "cards"
            st.session_state.selected_id = None
            st.rerun()

        item = df[df['writeup_id'] == st.session_state.selected_id]
        if item.empty:
            st.warning("æœªæ‰¾åˆ°è¯¥ Writeup")
            return
        item = item.iloc[0]

        # æ ‡é¢˜
        st.markdown(f"## {item['title']}")
        st.caption(f"*{item['description']}*")

        badge_col1, badge_col2, badge_col3 = st.columns(3)
        badge_col1.info(f"ğŸ“‚ **{get_category_cn(item['category'])}**")
        badge_col2.success(f"âœ“ ç½®ä¿¡åº¦: **{item['confidence']}**")
        badge_col3.warning(f"ğŸ‘¤ **{item['authors']}**")

        # ä¸¤åˆ—å¸ƒå±€
        left_col, right_col = st.columns([2, 1])

        with left_col:
            # å®Œæ•´å†…å®¹
            md_path = item.get('markdown_path')
            if pd.notna(md_path):
                full_path = f"kaggle_writeups_export/{md_path}"
                try:
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    st.markdown("### ğŸ“„ å®Œæ•´æè¿°")
                    with st.container(height=500):
                        st.markdown(content)
                except Exception as e:
                    st.warning(f"æ— æ³•åŠ è½½: {e}")

        with right_col:
            # YouTube åµŒå…¥
            yt_url = item.get('youtube_links')
            yt_id = extract_youtube_id(yt_url)
            if yt_id:
                st.markdown("### ğŸ¬ è§†é¢‘")
                st.markdown(f"""
                <iframe width="100%" height="200"
                    src="https://www.youtube.com/embed/{yt_id}"
                    frameborder="0" allowfullscreen>
                </iframe>
                """, unsafe_allow_html=True)

            # é“¾æ¥
            st.markdown("### ğŸ”— é“¾æ¥")
            st.link_button("ğŸŒ Kaggle Writeup", f"https://www.kaggle.com{item['url']}", use_container_width=True)

            app_links = item.get('application_links')
            if pd.notna(app_links) and app_links:
                for link in str(app_links).split(';')[:3]:
                    if link.strip():
                        st.link_button("ğŸš€ åº”ç”¨é“¾æ¥", link.strip(), use_container_width=True)

            # å…ƒæ•°æ®
            st.markdown("### ğŸ“‹ å…ƒæ•°æ®")
            st.json({
                "ID": str(item['writeup_id']),
                "åˆ›å»ºæ—¶é—´": str(item['create_time'])[:10],
                "æ›´æ–°æ—¶é—´": str(item['update_time'])[:10],
            })

    # å¡ç‰‡è§†å›¾
    elif st.session_state.view_mode == "cards":
        if filtered_df.empty:
            st.info("æ²¡æœ‰ç¬¦åˆç­›é€‰æ¡ä»¶çš„ Writeupã€‚")
            return

        # åˆ†é¡µ
        items_per_page = 24
        total_pages = (len(filtered_df) - 1) // items_per_page + 1

        page_col1, page_col2, page_col3 = st.columns([1, 2, 1])
        with page_col1:
            if st.button("â† ä¸Šä¸€é¡µ") and st.session_state.page > 0:
                st.session_state.page -= 1
                st.rerun()
        with page_col2:
            st.markdown(f"<center>ç¬¬ {st.session_state.page + 1} é¡µ / å…± {total_pages} é¡µ</center>", unsafe_allow_html=True)
        with page_col3:
            if st.button("ä¸‹ä¸€é¡µ â†’") and st.session_state.page < total_pages - 1:
                st.session_state.page += 1
                st.rerun()

        # ç½‘æ ¼æ˜¾ç¤ºå¡ç‰‡
        start_idx = st.session_state.page * items_per_page
        end_idx = min(start_idx + items_per_page, len(filtered_df))
        page_df = filtered_df.iloc[start_idx:end_idx]

        cols = st.columns(3)
        for idx, (_, row) in enumerate(page_df.iterrows()):
            with cols[idx % 3]:
                render_card(row, start_idx + idx)

    # è¡¨æ ¼è§†å›¾
    elif st.session_state.view_mode == "table":
        # åˆ›å»ºå‰¯æœ¬å¹¶è½¬æ¢åˆ†ç±»ä¸ºä¸­æ–‡
        table_cols = ['writeup_id', 'title', 'category', 'confidence', 'authors', 'description']
        # å¦‚æœæœ‰ä¸­æ–‡å…³é”®è¯åˆ—ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥
        if 'keywords_cn' in filtered_df.columns:
            table_cols.append('keywords_cn')
        table_df = filtered_df[table_cols].copy()
        table_df['category'] = table_df['category'].apply(get_category_cn)

        rename_cols = {
            'writeup_id': 'ID',
            'title': 'æ ‡é¢˜',
            'category': 'åˆ†ç±»',
            'confidence': 'ç½®ä¿¡åº¦',
            'authors': 'ä½œè€…',
            'description': 'æè¿°',
            'keywords_cn': 'ä¸­æ–‡å…³é”®è¯'
        }
        st.dataframe(
            table_df.rename(columns=rename_cols),
            use_container_width=True,
            height=600
        )

    # ç»Ÿè®¡è§†å›¾
    elif st.session_state.view_mode == "stats":
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡å…³é”®è¯åˆ—
        has_keywords_cn = 'keywords_cn' in filtered_df.columns

        # ä¸­æ–‡åœç”¨è¯åˆ—è¡¨ï¼ˆè¿‡æ»¤æ‰è¿‡äºé€šç”¨çš„è¯ï¼‰
        STOPWORDS_CN = {
            # AI/æŠ€æœ¯é€šç”¨è¯
            'äººå·¥æ™ºèƒ½', 'AI', 'AIåŠ©æ‰‹', 'AIä»£ç†', 'AIå¯¼å¸ˆ', 'AIæ•™è‚²',
            'æ™ºèƒ½åŠ©æ‰‹', 'æ™ºèƒ½ä»£ç†', 'æ™ºèƒ½è¾…å¯¼', 'æ™ºèƒ½è¯Šæ–­',
            # Gemini ç›¸å…³
            'Gemini', 'Geminiæ¨¡å‹', 'GeminiæŠ€æœ¯', 'Gemini AI', 'Geminié©±åŠ¨',
            'Gemini 3 Pro', 'Gemini 3', 'Gemini Pro',
            # å¤šæ¨¡æ€ç›¸å…³
            'å¤šæ¨¡æ€AI', 'å¤šæ¨¡æ€', 'å¤šæ¨¡æ€æ¨ç†',
            # å…¶ä»–é€šç”¨è¯
            'æ•°æ®åˆ†æ', 'æ•ˆç‡æå‡', 'ç§»åŠ¨åº”ç”¨', 'å¼€å‘å·¥å…·',
            'å†³ç­–æ”¯æŒ', 'çŸ¥è¯†ç®¡ç†', 'åˆ›æ„å·¥å…·',
        }

        # è¯äº‘ç”Ÿæˆè¾…åŠ©å‡½æ•°ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        def generate_wordcloud(text, colormap='viridis', is_chinese=False):
            if not text.strip():
                return None
            # ä¸­æ–‡è¯äº‘éœ€è¦æŒ‡å®šå­—ä½“
            font_path = None
            if is_chinese:
                # å°è¯•å¸¸è§çš„ä¸­æ–‡å­—ä½“è·¯å¾„
                chinese_fonts = [
                    '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',
                    '/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc',
                    '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',
                    '/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf',
                    '/System/Library/Fonts/PingFang.ttc',  # macOS
                    'C:/Windows/Fonts/msyh.ttc',  # Windows
                ]
                import os
                for fp in chinese_fonts:
                    if os.path.exists(fp):
                        font_path = fp
                        break

            return WordCloud(
                width=800,
                height=400,
                background_color='white',
                colormap=colormap,
                relative_scaling=0.5,
                min_font_size=8,
                max_words=80,
                prefer_horizontal=0.7,
                font_path=font_path
            ).generate(text)

        # è·å–è¯äº‘æ–‡æœ¬çš„è¾…åŠ©å‡½æ•°
        def get_wordcloud_text(df_subset):
            """ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡å…³é”®è¯ï¼Œå¦åˆ™ä½¿ç”¨æ ‡é¢˜"""
            if has_keywords_cn:
                keywords = df_subset['keywords_cn'].dropna().astype(str).tolist()
                # å…³é”®è¯æ˜¯é€—å·åˆ†éš”çš„ï¼Œè¿‡æ»¤åœç”¨è¯åè½¬æ¢ä¸ºç©ºæ ¼åˆ†éš”
                filtered_words = []
                for kw in keywords:
                    words = [w.strip() for w in kw.split(',') if w.strip()]
                    # è¿‡æ»¤åœç”¨è¯
                    words = [w for w in words if w not in STOPWORDS_CN]
                    filtered_words.extend(words)
                text = ' '.join(filtered_words)
                if text.strip():
                    return text, True  # è¿”å›æ–‡æœ¬å’Œæ˜¯å¦ä¸ºä¸­æ–‡
            # å›é€€åˆ°è‹±æ–‡æ ‡é¢˜
            return ' '.join(df_subset['title'].dropna().astype(str).tolist()), False

        # æ€»è¯äº‘éƒ¨åˆ†
        if has_keywords_cn:
            st.markdown("### â˜ï¸ æ€»è¯äº‘ï¼ˆä¸­æ–‡å…³é”®è¯ï¼‰")
        else:
            st.markdown("### â˜ï¸ æ€»è¯äº‘ï¼ˆè‹±æ–‡æ ‡é¢˜ï¼‰")

        all_text, is_chinese = get_wordcloud_text(filtered_df)

        if all_text.strip():
            total_wc = generate_wordcloud(all_text, 'viridis', is_chinese)
            if total_wc:
                # é‡æ–°ç”Ÿæˆå¤§å°ºå¯¸è¯äº‘
                font_path = total_wc.font_path
                total_wc = WordCloud(
                    width=1600,
                    height=600,
                    background_color='white',
                    colormap='viridis',
                    relative_scaling=0.5,
                    min_font_size=10,
                    max_words=150,
                    prefer_horizontal=0.7,
                    font_path=font_path
                ).generate(all_text)

                fig, ax = plt.subplots(figsize=(16, 6))
                ax.imshow(total_wc, interpolation='bilinear')
                ax.axis('off')
                plt.tight_layout(pad=0)
                st.pyplot(fig)
                plt.close()
        else:
            st.info("æ²¡æœ‰å¯ç”¨äºç”Ÿæˆè¯äº‘çš„æ•°æ®")

        st.markdown("---")

        # æŒ‰åˆ†ç±»çš„è¯äº‘
        st.markdown("### â˜ï¸ åˆ†ç±»è¯äº‘")

        categories = filtered_df['category'].dropna().unique().tolist()
        colormaps = ['viridis', 'plasma', 'inferno', 'magma', 'cividis', 'cool', 'spring', 'summer', 'autumn', 'winter']

        # ä¸¤åˆ—ç½‘æ ¼æ˜¾ç¤º
        cols = st.columns(2)
        for idx, category in enumerate(sorted(categories)):
            cat_df = filtered_df[filtered_df['category'] == category]
            cat_text, is_chinese = get_wordcloud_text(cat_df)

            if cat_text.strip():
                with cols[idx % 2]:
                    st.markdown(f"**{get_category_cn(category)}**ï¼ˆ{len(cat_df)} æ¡ï¼‰")
                    wc = generate_wordcloud(cat_text, colormaps[idx % len(colormaps)], is_chinese)
                    if wc:
                        fig, ax = plt.subplots(figsize=(8, 4))
                        ax.imshow(wc, interpolation='bilinear')
                        ax.axis('off')
                        plt.tight_layout(pad=0)
                        st.pyplot(fig)
                        plt.close()

        st.markdown("---")

        stat_col1, stat_col2 = st.columns(2)

        with stat_col1:
            st.markdown("### ğŸ“‚ åˆ†ç±»åˆ†å¸ƒ")
            counts = filtered_df['category'].value_counts().reset_index()
            counts.columns = ['category', 'æ•°é‡']
            counts['åˆ†ç±»'] = counts['category'].apply(get_category_cn)
            fig1 = px.bar(counts, x='åˆ†ç±»', y='æ•°é‡', color='æ•°é‡', height=500)
            fig1.update_layout(xaxis_tickangle=-45)
            st.plotly_chart(fig1, use_container_width=True)

        with stat_col2:
            st.markdown("### âœ“ ç½®ä¿¡åº¦åˆ†å¸ƒ")
            conf_counts = filtered_df['confidence'].value_counts().reset_index()
            conf_counts.columns = ['ç½®ä¿¡åº¦', 'æ•°é‡']
            fig2 = px.pie(conf_counts, values='æ•°é‡', names='ç½®ä¿¡åº¦', height=400)
            st.plotly_chart(fig2, use_container_width=True)

            st.markdown("### ğŸ† çƒ­é—¨ä½œè€… Top 10")
            author_counts = filtered_df['authors'].value_counts().head(10).reset_index()
            author_counts.columns = ['ä½œè€…', 'æ•°é‡']
            st.dataframe(author_counts, use_container_width=True, hide_index=True)

if __name__ == "__main__":
    main()
